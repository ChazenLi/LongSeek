"""
Identifiability Diagnostics for CA Models (Week 6)

This module provides tools for assessing parameter identifiability:
1. Posterior correlation matrix - reveals parameter correlations
2. Profile likelihood analysis - assesses single-parameter identifiability
3. Fisher information matrix - quantifies parameter precision
4. Sensitivity analysis - evaluates parameter influence on outputs

Key Insight: A parameter is "identifiable" if:
- Profile likelihood has a unique peak (not flat)
- Posterior correlation with other params is low (|corr| < 0.5)
- Fisher information is high (I_ii > 0.01)
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass
import logging
from scipy.optimize import minimize
from scipy.stats import pearsonr
from scipy.ndimage import gaussian_filter

logger = logging.getLogger(__name__)


@dataclass
class IdentifiabilityResult:
    """å¯è¾¨è¯†æ€§åˆ†æžç»“æžœ"""
    parameter_name: str
    identifiable: bool
    profile_likelihood: np.ndarray
    param_range: np.ndarray
    mle: float
    confidence_interval: Tuple[float, float]
    fisher_info: float
    posterior_corr: Dict[str, float]


def compute_posterior_correlation(
    params_history: List[Dict],
    param_names: Optional[List[str]] = None
) -> Tuple[np.ndarray, List[str]]:
    """
    è®¡ç®—åŽéªŒç›¸å…³çŸ©é˜µ

    Args:
        params_history: å‚æ•°åŽ†å²è®°å½• (æ¯æ¬¡è¿­ä»£çš„å‚æ•°å€¼)
        param_names: å‚æ•°åç§°åˆ—è¡¨

    Returns:
        correlation_matrix: ç›¸å…³çŸ©é˜µ (n_params, n_params)
        param_names: å‚æ•°åç§°
    """
    if param_names is None:
        param_names = list(params_history[0].keys())

    # æå–å‚æ•°å€¼
    n_samples = len(params_history)
    n_params = len(param_names)

    param_matrix = np.zeros((n_samples, n_params))
    for i, params in enumerate(params_history):
        for j, name in enumerate(param_names):
            param_matrix[i, j] = params.get(name, 0.0)

    # è®¡ç®—ç›¸å…³çŸ©é˜µ
    correlation_matrix = np.corrcoef(param_matrix.T)

    return correlation_matrix, param_names


def compute_profile_likelihood(
    param_name: str,
    param_range: np.ndarray,
    fixed_params: Dict[str, float],
    objective_fn: Callable,
    baseline_loss: float,
    n_points: int = 20
) -> Tuple[np.ndarray, np.ndarray, float, Tuple[float, float]]:
    """
    è®¡ç®—å•å‚æ•°å‰–é¢ä¼¼ç„¶

    åŽŸç†ï¼šå›ºå®šå…¶ä»–å‚æ•°åœ¨MLEå€¼ï¼Œæ‰«æç›®æ ‡å‚æ•°ï¼Œè§‚å¯Ÿlosså˜åŒ–

    Args:
        param_name: ç›®æ ‡å‚æ•°å
        param_range: å‚æ•°æ‰«æèŒƒå›´
        fixed_params: å›ºå®šçš„å…¶ä»–å‚æ•°
        objective_fn: ç›®æ ‡å‡½æ•°
        baseline_loss: MLEå¤„çš„losså€¼
        n_points: æ‰«æç‚¹æ•°

    Returns:
        param_values: æ‰«æçš„å‚æ•°å€¼
        likelihood_values: å¯¹åº”çš„ä¼¼ç„¶å€¼ (-loss)
        mle: æœ€å¤§ä¼¼ç„¶ä¼°è®¡å€¼
        confidence_interval: 95%ç½®ä¿¡åŒºé—´
    """
    param_values = np.linspace(param_range[0], param_range[1], n_points)
    likelihood_values = np.zeros(n_points)

    for i, val in enumerate(param_name, param_values):
        # æž„é€ å‚æ•°
        params = fixed_params.copy()
        params[param_name] = val

        # è®¡ç®—loss
        try:
            loss = objective_fn(params)
            likelihood_values[i] = -loss
        except Exception as e:
            logger.warning(f"Failed to evaluate at {param_name}={val}: {e}")
            likelihood_values[i] = -np.inf

    # å½’ä¸€åŒ–ä¼¼ç„¶
    likelihood_values = likelihood_values - np.max(likelihood_values)

    # æ‰¾åˆ°MLE
    mle_idx = np.argmax(likelihood_values)
    mle = param_values[mle_idx]

    # è®¡ç®—ç½®ä¿¡åŒºé—´ (likelihood threshold = -1.92 for 95% CI)
    threshold = -1.92
    valid_mask = likelihood_values >= threshold

    if np.any(valid_mask):
        valid_params = param_values[valid_mask]
        confidence_interval = (float(valid_params.min()), float(valid_params.max()))
    else:
        confidence_interval = (mle, mle)

    return param_values, likelihood_values, mle, confidence_interval


def compute_fisher_information(
    objective_fn: Callable,
    params: Dict[str, float],
    param_names: List[str],
    epsilon: float = 1e-6
) -> Tuple[np.ndarray, np.ndarray]:
    """
    è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ

    åŽŸç†ï¼šFisherä¿¡æ¯ = HessiançŸ©é˜µçš„è´Ÿé€†
    I_ij = E[âˆ‚Â²L/âˆ‚Î¸_iâˆ‚Î¸_j]

    Args:
        objective_fn: ç›®æ ‡å‡½æ•°
        params: å‚æ•°å­—å…¸
        param_names: å‚æ•°åç§°åˆ—è¡¨
        epsilon: æœ‰é™å·®åˆ†æ­¥é•¿

    Returns:
        fisher_info: Fisherä¿¡æ¯çŸ©é˜µ (n_params, n_params)
        eigenvalues: ç‰¹å¾å€¼ï¼ˆå‚æ•°ç²¾åº¦æŒ‡æ ‡ï¼‰
    """
    n_params = len(param_names)
    hessian = np.zeros((n_params, n_params))

    # è½¬æ¢ä¸ºå‘é‡
    param_vector = np.array([params[name] for name in param_names])

    # è®¡ç®—Hessianï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
    base_loss = objective_fn(params)

    for i in range(n_params):
        for j in range(n_params):
            # äºŒé˜¶ä¸­å¿ƒå·®åˆ†
            params_ij = params.copy()

            if i == j:
                # å¯¹è§’å…ƒç´ ï¼šf(x+h) - 2f(x) + f(x-h)
                params_ij[param_names[i]] = param_vector[i] + epsilon
                f_plus = objective_fn(params_ij)

                params_ij[param_names[i]] = param_vector[i] - epsilon
                f_minus = objective_fn(params_ij)

                hessian[i, j] = (f_plus - 2 * base_loss + f_minus) / (epsilon ** 2)
            else:
                # éžå¯¹è§’å…ƒç´ ï¼šæ··åˆå¯¼æ•°
                params_ij[param_names[i]] = param_vector[i] + epsilon
                params_ij[param_names[j]] = param_vector[j] + epsilon
                f_pp = objective_fn(params_ij)

                params_ij[param_names[i]] = param_vector[i] - epsilon
                f_mp = objective_fn(params_ij)

                params_ij[param_names[i]] = param_vector[i] + epsilon
                params_ij[param_names[j]] = param_vector[j] - epsilon
                f_pm = objective_fn(params_ij)

                params_ij[param_names[i]] = param_vector[i] - epsilon
                params_ij[param_names[j]] = param_vector[j] - epsilon
                f_mm = objective_fn(params_ij)

                hessian[i, j] = (f_pp - f_mp - f_pm + f_mm) / (4 * epsilon ** 2)

    # Fisherä¿¡æ¯ = Hessiançš„è´Ÿé€†ï¼ˆåœ¨æœ€ä¼˜è§£é™„è¿‘ï¼‰
    try:
        fisher_info = -np.linalg.inv(hessian)
        eigenvalues, _ = np.linalg.eig(fisher_info)
    except np.linalg.LinAlgError:
        logger.warning("Hessian is singular, using pseudo-inverse")
        fisher_info = -np.linalg.pinv(hessian)
        eigenvalues, _ = np.linalg.eig(fisher_info)

    return fisher_info, eigenvalues


def compute_sensitivity_indices(
    params: Dict[str, float],
    param_ranges: Dict[str, Tuple[float, float]],
    model_fn: Callable,
    output_names: List[str],
    n_samples: int = 100
) -> Dict[str, Dict[str, float]]:
    """
    è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆSobolæŒ‡æ•°è¿‘ä¼¼ï¼‰

    åŽŸç†ï¼šå‚æ•°æ‰°åŠ¨å¯¹è¾“å‡ºçš„å½±å“ç¨‹åº¦

    Args:
        params: åŸºå‡†å‚æ•°
        param_ranges: å‚æ•°èŒƒå›´
        model_fn: æ¨¡åž‹å‡½æ•°
        output_names: è¾“å‡ºåç§°
        n_samples: é‡‡æ ·æ•°

    Returns:
        sensitivity_indices: {param: {output: sensitivity}}
    """
    param_names = list(param_ranges.keys())
    n_params = len(param_names)

    # èŽ·å–åŸºå‡†è¾“å‡º
    baseline_outputs = model_fn(params)

    # è®¡ç®—æ•æ„Ÿæ€§
    sensitivity = {name: {} for name in param_names}

    for param_name in param_names:
        base_val = params[param_name]
        min_val, max_val = param_ranges[param_name]
        delta = (max_val - min_val) * 0.1  # 10%æ‰°åŠ¨

        # æ­£å‘æ‰°åŠ¨
        params_plus = params.copy()
        params_plus[param_name] = base_val + delta
        outputs_plus = model_fn(params_plus)

        # è´Ÿå‘æ‰°åŠ¨
        params_minus = params.copy()
        params_minus[param_name] = max(base_val - delta, min_val)
        outputs_minus = model_fn(params_minus)

        # è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ•° (å½’ä¸€åŒ–)
        for output_name in output_names:
            baseline_val = baseline_outputs.get(output_name, 0.0)
            output_plus = outputs_plus.get(output_name, 0.0)
            output_minus = outputs_minus.get(output_name, 0.0)

            # ä¸­å¿ƒå·®åˆ†
            diff = (output_plus - output_minus) / 2
            normalized = abs(diff / (abs(baseline_val) + 1e-10))

            sensitivity[param_name][output_name] = float(normalized)

    return sensitivity


class IdentifiabilityDiagnostics:
    """
    å¯è¾¨è¯†æ€§è¯Šæ–­å·¥å…·ç±»

    ä½¿ç”¨æ–¹æ³•ï¼š
        diagnostics = IdentifiabilityDiagnostics(
            objective_fn=loss_function,
            param_names=['p_move', 'p_div', 'alpha', 'beta', 'gamma', 'tau'],
            param_ranges={...}
        )

        results = diagnostics.analyze(params_history, best_params)
        diagnostics.plot_results(results)
        diagnostics.save_report(results, 'identifiability_report.json')
    """

    def __init__(self,
                 objective_fn: Callable,
                 param_names: List[str],
                 param_ranges: Dict[str, Tuple[float, float]]):
        """
        Args:
            objective_fn: ç›®æ ‡å‡½æ•°
            param_names: å‚æ•°åç§°åˆ—è¡¨
            param_ranges: å‚æ•°æœç´¢èŒƒå›´
        """
        self.objective_fn = objective_fn
        self.param_names = param_names
        self.param_ranges = param_ranges

    def analyze_correlation(self,
                          params_history: List[Dict]) -> Tuple[np.ndarray, Dict]:
        """
        åˆ†æžå‚æ•°ç›¸å…³æ€§

        Returns:
            correlation_matrix: ç›¸å…³çŸ©é˜µ
            high_correlations: é«˜ç›¸å…³å‚æ•°å¯¹ (|corr| > 0.5)
        """
        corr_matrix, names = compute_posterior_correlation(params_history, self.param_names)

        # æ‰¾å‡ºé«˜ç›¸å…³å‚æ•°å¯¹
        high_correlations = {}
        for i in range(len(names)):
            for j in range(i+1, len(names)):
                corr_val = corr_matrix[i, j]
                if abs(corr_val) > 0.5:
                    high_correlations[f"{names[i]}_{names[j]}"] = corr_val

        return corr_matrix, high_correlations

    def analyze_profile_likelihood(self,
                                   best_params: Dict,
                                   baseline_loss: float,
                                   n_points: int = 20) -> Dict[str, IdentifiabilityResult]:
        """
        åˆ†æžæ¯ä¸ªå‚æ•°çš„å‰–é¢ä¼¼ç„¶

        Returns:
            results: {param_name: IdentifiabilityResult}
        """
        results = {}

        for param_name in self.param_names:
            param_range = self.param_ranges[param_name]

            # å›ºå®šå…¶ä»–å‚æ•°
            fixed_params = best_params.copy()

            # è®¡ç®—å‰–é¢ä¼¼ç„¶
            param_vals, likelihood, mle, ci = compute_profile_likelihood(
                param_name,
                param_range,
                fixed_params,
                self._param_wrapper(fixed_params),
                baseline_loss,
                n_points
            )

            # åˆ¤æ–­å¯è¾¨è¯†æ€§ï¼šæ˜¯å¦æœ‰æ¸…æ™°å³°å€¼
            # ä½¿ç”¨likelihoodå˜åŒ–èŒƒå›´åˆ¤æ–­
            likelihood_range = likelihood.max() - likelihood.min()
            identifiable = likelihood_range > 2.0  # é˜ˆå€¼ï¼š2ä¸ªlogå•ä½

            results[param_name] = IdentifiabilityResult(
                parameter_name=param_name,
                identifiable=identifiable,
                profile_likelihood=likelihood,
                param_range=param_vals,
                mle=mle,
                confidence_interval=ci,
                fisher_info=0.0,  # åŽç»­å¡«å……
                posterior_corr={}  # åŽç»­å¡«å……
            )

        return results

    def analyze_fisher_information(self,
                                   best_params: Dict) -> Tuple[np.ndarray, np.ndarray, Dict]:
        """
        è®¡ç®—Fisherä¿¡æ¯çŸ©é˜µ

        Returns:
            fisher_info: Fisherä¿¡æ¯çŸ©é˜µ
            eigenvalues: ç‰¹å¾å€¼
            precision: å‚æ•°ç²¾åº¦æŒ‡æ ‡
        """
        fisher_info, eigenvalues = compute_fisher_information(
            self._param_wrapper(best_params),
            best_params,
            self.param_names
        )

        # æå–å¯¹è§’çº¿å…ƒç´ ä½œä¸ºå‚æ•°ç²¾åº¦
        precision = {}
        for i, name in enumerate(self.param_names):
            precision[name] = fisher_info[i, i]

        return fisher_info, eigenvalues, precision

    def analyze_sensitivity(self,
                          best_params: Dict,
                          model_fn: Callable,
                          n_samples: int = 50) -> Dict[str, Dict[str, float]]:
        """
        å‚æ•°æ•æ„Ÿæ€§åˆ†æž

        Returns:
            sensitivity: {param: {output: sensitivity_index}}
        """
        return compute_sensitivity_indices(
            best_params,
            self.param_ranges,
            model_fn,
            ['wound_area', 'migrations', 'divisions'],
            n_samples
        )

    def full_analysis(self,
                     params_history: List[Dict],
                     best_params: Dict,
                     baseline_loss: float,
                     model_fn: Optional[Callable] = None) -> Dict:
        """
        å®Œæ•´çš„å¯è¾¨è¯†æ€§åˆ†æž

        Returns:
            analysis_results: åŒ…å«æ‰€æœ‰åˆ†æžç»“æžœ
        """
        results = {
            'correlation': {},
            'profile_likelihood': {},
            'fisher_info': {},
            'sensitivity': {},
            'summary': {}
        }

        # 1. ç›¸å…³æ€§åˆ†æž
        logger.info("Computing posterior correlation matrix...")
        corr_matrix, high_corr = self.analyze_correlation(params_history)
        results['correlation']['matrix'] = corr_matrix
        results['correlation']['high_correlations'] = high_corr

        # 2. å‰–é¢ä¼¼ç„¶åˆ†æž
        logger.info("Computing profile likelihood...")
        profile_results = self.analyze_profile_likelihood(best_params, baseline_loss)
        results['profile_likelihood']['results'] = profile_results

        # 3. Fisherä¿¡æ¯
        logger.info("Computing Fisher information...")
        fisher_info, eigenvalues, precision = self.analyze_fisher_information(best_params)
        results['fisher_info']['matrix'] = fisher_info
        results['fisher_info']['eigenvalues'] = eigenvalues
        results['fisher_info']['precision'] = precision

        # 4. æ•æ„Ÿæ€§åˆ†æž
        if model_fn is not None:
            logger.info("Computing sensitivity indices...")
            sensitivity = self.analyze_sensitivity(best_params, model_fn)
            results['sensitivity']['indices'] = sensitivity

        # 5. æ±‡æ€»
        results['summary'] = self._generate_summary(
            corr_matrix, profile_results, precision, high_corr
        )

        return results

    def _generate_summary(self,
                         corr_matrix: np.ndarray,
                         profile_results: Dict[str, IdentifiabilityResult],
                         precision: Dict[str, float],
                         high_corr: Dict) -> Dict:
        """ç”Ÿæˆå¯è¾¨è¯†æ€§æ±‡æ€»"""
        summary = {
            'identifiable_params': [],
            'weakly_identifiable_params': [],
            'unidentifiable_params': [],
            'highly_correlated_pairs': high_corr,
            'recommendations': []
        }

        for param_name in self.param_names:
            result = profile_results[param_name]
            prec = precision.get(param_name, 0.0)

            # åˆ¤æ–­æ ‡å‡†
            if result.identifiable and prec > 0.01:
                summary['identifiable_params'].append(param_name)
            elif result.identifiable or prec > 0.001:
                summary['weakly_identifiable_params'].append(param_name)
            else:
                summary['unidentifiable_params'].append(param_name)

        # ç”Ÿæˆå»ºè®®
        if len(summary['unidentifiable_params']) > 0:
            summary['recommendations'].append(
                f"Unidentifiable parameters: {summary['unidentifiable_params']}. "
                "Consider fixing these or collecting more data."
            )

        if len(high_corr) > 0:
            summary['recommendations'].append(
                f"High correlations detected: {list(high_corr.keys())}. "
                "Consider reparameterization or adding constraints."
            )

        return summary

    def _param_wrapper(self, fixed_params: Dict) -> Callable:
        """åŒ…è£…ç›®æ ‡å‡½æ•°ï¼ŒæŽ¥å—å‚æ•°å­—å…¸"""
        def wrapper(params_array: np.ndarray) -> float:
            params = fixed_params.copy()
            for i, name in enumerate(self.param_names):
                params[name] = params_array[i]
            return self.objective_fn(params)
        return wrapper

    def save_report(self, results: Dict, filepath: str):
        """ä¿å­˜åˆ†æžæŠ¥å‘Šä¸ºJSON"""
        import json
        from datetime import datetime

        # è½¬æ¢numpyç±»åž‹ä¸ºPythonåŽŸç”Ÿç±»åž‹
        def convert(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.floating)):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert(v) for v in obj]
            elif isinstance(obj, IdentifiabilityResult):
                return {
                    'parameter_name': obj.parameter_name,
                    'identifiable': obj.identifiable,
                    'mle': obj.mle,
                    'confidence_interval': obj.confidence_interval,
                    'fisher_info': obj.fisher_info,
                }
            return obj

        report = {
            'timestamp': datetime.now().isoformat(),
            'results': convert(results),
            'param_names': self.param_names,
        }

        with open(filepath, 'w') as f:
            json.dump(report, f, indent=2)

        logger.info(f"Identifiability report saved to {filepath}")


def plot_identifiability_results(results: Dict, save_path: Optional[str] = None):
    """
    å¯è§†åŒ–å¯è¾¨è¯†æ€§åˆ†æžç»“æžœ

    Args:
        results: full_analysisè¿”å›žçš„ç»“æžœ
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    import matplotlib.pyplot as plt
    from matplotlib.gridspec import GridSpec

    param_names = results.get('param_names', [])
    corr_matrix = results['correlation']['matrix']
    profile_results = results['profile_likelihood'].get('results', {})

    fig = plt.figure(figsize=(16, 10))
    gs = GridSpec(2, 3, figure=fig)

    # 1. åŽéªŒç›¸å…³çŸ©é˜µçƒ­åŠ›å›¾
    ax1 = fig.add_subplot(gs[0, 0])
    im = ax1.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
    ax1.set_xticks(range(len(param_names)))
    ax1.set_yticks(range(len(param_names)))
    ax1.set_xticklabels(param_names, rotation=45, ha='right')
    ax1.set_yticklabels(param_names)
    ax1.set_title('Posterior Correlation Matrix')
    plt.colorbar(im, ax=ax1)

    # æ·»åŠ ç›¸å…³ç³»æ•°æ ‡æ³¨
    for i in range(len(param_names)):
        for j in range(len(param_names)):
            text = ax1.text(j, i, f'{corr_matrix[i, j]:.2f}',
                          ha='center', va='center', fontsize=8)

    # 2. å‰–é¢ä¼¼ç„¶æ›²çº¿
    ax2 = fig.add_subplot(gs[0, 1:])
    for param_name, result in profile_results.items():
        if hasattr(result, 'param_range') and hasattr(result, 'profile_likelihood'):
            ax2.plot(result.param_range, result.profile_likelihood,
                    label=param_name, marker='o', markersize=3)

    ax2.axhline(y=-1.92, color='r', linestyle='--', label='95% CI threshold')
    ax2.set_xlabel('Parameter Value')
    ax2.set_ylabel('Log-Likelihood')
    ax2.set_title('Profile Likelihood')
    ax2.legend(ncol=3, fontsize=8)
    ax2.grid(True, alpha=0.3)

    # 3. Fisherä¿¡æ¯ï¼ˆæ¡å½¢å›¾ï¼‰
    ax3 = fig.add_subplot(gs[1, 0])
    fisher_info = results['fisher_info'].get('precision', {})
    if fisher_info:
        names = list(fisher_info.keys())
        values = [fisher_info[n] for n in names]
        colors = ['green' if v > 0.01 else 'orange' if v > 0.001 else 'red' for v in values]
        ax3.barh(names, values, color=colors)
        ax3.axvline(x=0.01, color='r', linestyle='--', label='High precision threshold')
        ax3.set_xlabel('Fisher Information (Diagonal)')
        ax3.set_title('Parameter Precision')
        ax3.legend()

    # 4. å¯è¾¨è¯†æ€§æ±‡æ€»
    ax4 = fig.add_subplot(gs[1, 1:])
    ax4.axis('off')

    summary = results.get('summary', {})
    text = "IDENTIFIABILITY SUMMARY\n\n"

    text += "âœ… Identifiable:\n"
    for p in summary.get('identifiable_params', []):
        text += f"  - {p}\n"

    text += "\nâš ï¸ Weakly Identifiable:\n"
    for p in summary.get('weakly_identifiable_params', []):
        text += f"  - {p}\n"

    text += "\nâŒ Unidentifiable:\n"
    for p in summary.get('unidentifiable_params', []):
        text += f"  - {p}\n"

    text += "\nðŸ“— Recommendations:\n"
    for rec in summary.get('recommendations', []):
        text += f"  - {rec}\n"

    ax4.text(0.05, 0.95, text, transform=ax4.transAxes,
            verticalalignment='top', fontsize=10,
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        logger.info(f"Identifiability plot saved to {save_path}")

    return fig


if __name__ == "__main__":
    # æµ‹è¯•å¯è¾¨è¯†æ€§è¯Šæ–­
    print("Testing Identifiability Diagnostics...")

    # æ¨¡æ‹Ÿå‚æ•°åŽ†å²
    np.random.seed(42)
    n_samples = 100

    param_names = ['p_move', 'p_div', 'alpha', 'beta', 'gamma', 'tau']
    params_history = []

    for _ in range(n_samples):
        params = {
            'p_move': np.random.uniform(0.3, 0.7),
            'p_div': np.random.uniform(0.01, 0.1),
            'alpha': np.random.uniform(0.5, 2.0),
            'beta': np.random.uniform(0.5, 2.0),
            'gamma': np.random.uniform(0.5, 1.5),
            'tau': np.random.randint(0, 3),
        }
        params_history.append(params)

    # è®¡ç®—ç›¸å…³çŸ©é˜µ
    corr_matrix, names = compute_posterior_correlation(params_history, param_names)
    print("\nPosterior Correlation Matrix:")
    print(corr_matrix)

    print("\nHigh correlations (|corr| > 0.5):")
    for i in range(len(names)):
        for j in range(i+1, len(names)):
            if abs(corr_matrix[i, j]) > 0.5:
                print(f"  {names[i]} - {names[j]}: {corr_matrix[i, j]:.3f}")

    print("\nIdentifiability diagnostics test complete!")
